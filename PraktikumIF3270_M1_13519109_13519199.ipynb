{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan informasi WHO, stroke merupakan penyebab kematian terbanyak nomor 2 di dunia dan menjadi penyebab dari 11% total kematian. [Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset) ini digunakan untuk memprediksi apakah seorang pasien memiliki kemungkinan besar untuk terkena stroke berdasarkan informasi tentang pasien seperti jenis kelamin, umur, penyakit dan status merokok.\n",
    "\n",
    "Tujuan eksperimen:\n",
    "1.   Peserta memahami rangkaian proses analitika data menggunakan pendekatan pembelajaran mesin. \n",
    "2.   Peserta memahami bahwa proses pengembangan model pembelajaran mesin juga ditentukan dari kualitas data, penanganan data, dan penentuan algoritma serta hiperparameternya; tidak cukup hanya dengan memastikan implementasi algoritma berjalan tanpa kesalahan.\n",
    "3. Peserta mampu menginterpretasikan hasil dari evaluasi model dalam proses analitika menggunakan pendekatan pembelajaran mesin.\n",
    "\n",
    "Praktikum dilaksanakan secara berkelompok, dengan 1 kelompok terdiri atas 2 mahasiswa. Soal praktikum terdapat di bagian bawah berkas ini. Harap diperhatikan bahwa terdapat berkas yang harus dikumpulkan sebelum waktu praktikum selesai (4 April 2022 11.00 WIB) dan berkas yang dikumpulkan setelah waktu praktikum selesai (4 April 2022 23.59 WIB). Untuk detil deliverables dan soal, dapat dilihat pada bagian bawah notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "X = data.drop(columns=\"stroke\")\n",
    "y = data[\"stroke\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "# data['Residence_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soal Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disediakan data yang sudah dibagi menjadi data train (df_train), validasi (df_val), dan test (df_test). Lakukanlah:\n",
    "1. Buatlah baseline dengan menggunakan model Logistic Regression\n",
    "2. Lakukan analisa data terkait:\n",
    "- Duplicate value\n",
    "- Missing value\n",
    "- Outlier\n",
    "- Balance of data\n",
    "3. Jelaskan bagaimana kalian akan menangani permasalahan yang disebutkan pada poin 2\n",
    "4. Sebutkan dan jelaskan alasan dari teknik encoding yang akan kalian gunakan terhadap data tersebut\n",
    "5. Buatlah desain eksperimen dengan menentukan hal berikut:\n",
    "- Tujuan eksperimen\n",
    "- Dependent dan Independent variabel\n",
    "- Strategi eksperimen\n",
    "- Skema validasi\n",
    "6. Implementasikan strategi eksperimen dan skema validasi yang sudah kalian buat\n",
    "7. Berdasarkan hasil prediksi yang kalian hasilkan, buatlah kesimpulan analisis karakteristik pasien yang terkena stroke\n",
    "\n",
    "Poin 1 - 5 dikerjakan saat praktikum berlangsung (pukul 09.00 WIB - 11.00 WIB)\n",
    "Poin 6 - 7 dikerjakan saat setelah praktikum berlangsung (pukul 11.00 WIB - 23.59 WIB)\n",
    "\n",
    "Jika terdapat perubahan jawaban pada poin 1 - 5 (semisal perbedaan cara melakukan handling missing value), dapat dijelaskan pada laporan mengenai jawaban sebelum, jawaban sesudah, dan alasan merubah jawaban tersebut (semisal menemukan suatu hal menarik pada data, sehingga missing value dapat dihandle dengan metode yang lebih bagus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9510763209393346\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_logistic = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "df = data.copy()\n",
    "# Melakukan Encoding\n",
    "categ = [\"gender\", \"ever_married\", \"smoking_status\"]\n",
    "le = LabelEncoder()\n",
    "df[categ] = df[categ].apply(le.fit_transform)\n",
    "\n",
    "oh = OneHotEncoder()\n",
    "categ = [\"work_type\", \"Residence_type\"]\n",
    "df[categ] = df[categ].apply(le.fit_transform)\n",
    "# Scaling avg_glucose_level dan bmi\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[[\"avg_glucose_level\"]])\n",
    "df[[\"avg_glucose_level\"]] = scaler.transform(df[[\"avg_glucose_level\"]])\n",
    "\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "df[\"bmi\"].fillna(df[\"bmi\"].mean(), inplace=True)\n",
    "scaler.fit(df[[\"bmi\"]])\n",
    "df[[\"bmi\"]] = scaler.transform(df[[\"bmi\"]])\n",
    "# Fit \n",
    "X_log = df.drop([\"id\",\"stroke\"], axis=1)\n",
    "y_log = df[\"stroke\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_log, y_log, test_size=0.2, random_state=123)\n",
    "model_logistic.fit(X_train,y_train)\n",
    "# Predict and evaluate\n",
    "y_pred = model_logistic.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "id mempunyai 0 missing values\n",
      "gender mempunyai 0 missing values\n",
      "age mempunyai 0 missing values\n",
      "hypertension mempunyai 0 missing values\n",
      "heart_disease mempunyai 0 missing values\n",
      "ever_married mempunyai 0 missing values\n",
      "work_type mempunyai 0 missing values\n",
      "Residence_type mempunyai 0 missing values\n",
      "avg_glucose_level mempunyai 0 missing values\n",
      "bmi mempunyai 201 missing values\n",
      "smoking_status mempunyai 0 missing values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "# Analisa duplicate value\n",
    "feature_col_num = data.select_dtypes(\n",
    "    include=[np.number]).columns.drop(\"stroke\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Missing Values\n",
    "print()\n",
    "feature_col = data.columns.drop(\"stroke\")\n",
    "for col in feature_col:\n",
    "    print(f\"{col} mempunyai {data[col].isnull().sum()} missing values\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "\n",
      "age\n",
      "\n",
      "hypertension\n",
      "1 is an outlier\n",
      "\n",
      "heart_disease\n",
      "1 is an outlier\n",
      "\n",
      "avg_glucose_level\n",
      "252.72 is an outlier\n",
      "243.58 is an outlier\n",
      "259.63 is an outlier\n",
      "249.31 is an outlier\n",
      "263.32 is an outlier\n",
      "271.74 is an outlier\n",
      "242.52 is an outlier\n",
      "250.89 is an outlier\n",
      "247.51 is an outlier\n",
      "243.53 is an outlier\n",
      "242.3 is an outlier\n",
      "243.5 is an outlier\n",
      "251.6 is an outlier\n",
      "247.69 is an outlier\n",
      "250.2 is an outlier\n",
      "254.6 is an outlier\n",
      "254.63 is an outlier\n",
      "246.34 is an outlier\n",
      "251.46 is an outlier\n",
      "267.76 is an outlier\n",
      "246.53 is an outlier\n",
      "244.28 is an outlier\n",
      "251.99 is an outlier\n",
      "253.16 is an outlier\n",
      "242.84 is an outlier\n",
      "249.29 is an outlier\n",
      "242.94 is an outlier\n",
      "247.48 is an outlier\n",
      "266.59 is an outlier\n",
      "243.73 is an outlier\n",
      "243.59 is an outlier\n",
      "250.8 is an outlier\n",
      "255.17 is an outlier\n",
      "267.61 is an outlier\n",
      "260.85 is an outlier\n",
      "248.37 is an outlier\n",
      "263.56 is an outlier\n",
      "247.97 is an outlier\n",
      "248.24 is an outlier\n",
      "253.93 is an outlier\n",
      "254.95 is an outlier\n",
      "247.87 is an outlier\n",
      "261.67 is an outlier\n",
      "256.74 is an outlier\n",
      "244.3 is an outlier\n",
      "242.62 is an outlier\n",
      "243.52 is an outlier\n",
      "267.6 is an outlier\n",
      "253.86 is an outlier\n",
      "\n",
      "bmi\n",
      "56.6 is an outlier\n",
      "54.6 is an outlier\n",
      "60.9 is an outlier\n",
      "54.7 is an outlier\n",
      "64.8 is an outlier\n",
      "60.2 is an outlier\n",
      "71.9 is an outlier\n",
      "55.7 is an outlier\n",
      "57.5 is an outlier\n",
      "54.2 is an outlier\n",
      "78.0 is an outlier\n",
      "53.4 is an outlier\n",
      "55.2 is an outlier\n",
      "55.0 is an outlier\n",
      "54.8 is an outlier\n",
      "52.8 is an outlier\n",
      "66.8 is an outlier\n",
      "55.1 is an outlier\n",
      "55.9 is an outlier\n",
      "57.3 is an outlier\n",
      "56.0 is an outlier\n",
      "57.7 is an outlier\n",
      "54.0 is an outlier\n",
      "56.1 is an outlier\n",
      "97.6 is an outlier\n",
      "53.9 is an outlier\n",
      "53.8 is an outlier\n",
      "52.7 is an outlier\n",
      "53.5 is an outlier\n",
      "63.3 is an outlier\n",
      "61.2 is an outlier\n",
      "58.1 is an outlier\n",
      "59.7 is an outlier\n",
      "52.5 is an outlier\n",
      "52.9 is an outlier\n",
      "61.6 is an outlier\n",
      "54.3 is an outlier\n",
      "57.2 is an outlier\n",
      "64.4 is an outlier\n",
      "92.0 is an outlier\n",
      "57.9 is an outlier\n",
      "54.1 is an outlier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outlier\n",
    "def detect_outlier(data_1):\n",
    "    threshold = 3\n",
    "    mean = np.mean(data_1)\n",
    "    std = np.std(data_1)\n",
    "    print(data_1.name)\n",
    "\n",
    "    for y in data_1.unique():\n",
    "        z_score = (y - mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            print(f\"{y} is an outlier\")\n",
    "    print()\n",
    "\n",
    "\n",
    "for col in feature_col_num:\n",
    "    detect_outlier(data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Training: Counter({0: 3109, 1: 161})\n",
      "Data Validation: Counter({0: 780, 1: 38})\n",
      "Data Testing: Counter({0: 972, 1: 50})\n"
     ]
    }
   ],
   "source": [
    "#Balance of Data\n",
    "print()\n",
    "print(f\"Data Training: {Counter(y_train)}\")\n",
    "print(f\"Data Validation: {Counter(y_val)}\")\n",
    "print(f\"Data Testing: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Jelaskan bagaimana kalian akan menangani permasalahan yang disebutkan pada poin 2\n",
    "4. Sebutkan dan jelaskan alasan dari teknik encoding yang akan kalian gunakan terhadap data tersebut\n",
    "5. Buatlah desain eksperimen dengan menentukan hal berikut:\n",
    "- Tujuan eksperimen\n",
    "- Dependent dan Independent variabel\n",
    "- Strategi eksperimen\n",
    "- Skema validasi\n",
    "\n",
    "3.  \n",
    "- Duplicate Values : Pertama drop kolom stroke, kemudian analisis duplicate values dengan fungsi df.duplicated(), akan me-return boolean values jika row dideteksi memiliki duplicate di tempat lain, lalu kita dapat melakukan penjumlahan terhadap boolean (0 dan 1) tersebut untuk mengetahui jumlah data yang terduplikat. Data duplikat tidak memberikan makna apa-apa ke pembelajaran, sehingga cara terbaik adalah dengan drop instance yang terdeteksi sebagai duplikat(keep 1 dari n-possible duplicate values)  \n",
    "    \n",
    "- Missing Values : Pertama drop kolom stroke, kemudian cari missing values dengan fungsi isnull, fungsi isnull() akan me-return boolean values true jika terdeteksi nilai fitur-j pada instance-i bernilai Nan, np.nan, string kosong, dan falsy-values lainnya. Lalu gunakan fungsi sum() untuk menambahkan seluruh boolean values dan mendapatkan jumlah missing values. Dikarenakan tidak terdapat missing values maka tidak perlu dilakukan penanganan khusus untuk dataset ini.\n",
    "    \n",
    "- Outlier : Pertama membuat fungsi deteksi outlier, fungsi ini akan melakukan list terhadap seluruh values yang terdeteksi pada outlier pada setiap kolom fitur. Syarat outlier adalah z_score dari value tersebut (selisih absolut value dengan mean dibagi standard deviation) lebih dari 3. Untuk mengatasi outlier ini bisa dilakukan normalisasi dengan MinMaxScaler atau log transformation, sehingga skewness dari dataset tidak condong terlalu ke kanan atau kek kiri.\n",
    "    \n",
    "- Balance of Data: Dengan library counter akan menghasilkan perbandingan jumlah dari value kolom target dari data training, data validation, dan data testing. Didapatkan ketidakseimbangan pada data target dalam dataset yang disediakan, dimana untuk data test, training, dan validation kelas 0 memiliki jumlah yang jauh lebih banyak dari kelas 1. Bisa melakukan pembelajaran dengan **undersampling** atau **oversampling** tergantung performa yang lebih baik untuk model yang sama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Untuk gender,dan ever_married dapat dilakukan label encoding, karena hanya memiliki 2 possible values,\n",
    "Sedangkan untuk Residence_type, job, dan smoking memiliki lebih dari 3 possible values, sehingga jauh lebih baik menggunakan OneHotEncoding karena mesin dapat lebih mengerti perbedaan keluaran dari OneHotEncoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. \n",
    "- Tujuan eksperimen : Mendeteksi seorang pasien apakah memiliki kemungkinan untuk terkena stroke berdasarkan informasi tentang pasien seperti jenis kelamin, umur, penyakit dan status merokok.\n",
    "\n",
    "- Dependent dan Independent variabel :\n",
    "    - Dependent :  jenis kelamin, umur, penyakit dan status merokok\n",
    "    - Independent : stroke\n",
    "    \n",
    "- Strategi eksperimen :\n",
    "    1. Melakukan cleaning terhadap data duplikat, handling missing values,dll.\n",
    "\n",
    "    2. Melakukan cek terhadap keseimbangan data, ketidakseimbangan memerlukan perlakuan khusus seperti under/over sampling, tergantung yang memberikan performa lebih baik.\n",
    "\n",
    "    3. Melakukan Encoding terhadap data kategorikal.\n",
    "\n",
    "    4. Melakukan cek jika terdapat outliers yang cukup signifikan pada kolom numerik, kemungkinannya adalah data mempunyai skewness yang terlalu tajam. Dapat dilakukan scaling dengan teknik MinMaxScaling atau Log Transformation, tergantung hasil terbaik.\n",
    "\n",
    "- Skema validasi :\n",
    "Validasi bisa dilakukan dengan menggunakan dataset validation dan dihitung f1 dan accuraccy score. Bisa juga melakukan library cross_val_score yang otomatis membagi dataset awal ke dalam beberapa fold, lalu menghitung score dari model untuk setiap fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelaskan pembagian tugas/ kerja antar anggota kelompok saat eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Notebook dengan nama file PraktikumIF3270_M1_NIM1_NIM2.ipynb untuk poin 1 - 5.\n",
    "2. Notebook dengan nama file PraktikumIF3270_M2_NIM1_NIM2.ipynb yang merupakan kelanjutan dari notebook poin 1, dengan tambahan hasil poin 6 dan 7.\n",
    "3. Laporan dengan nama file PraktikumIF3270_NIM1_NIM2.pdf dengan isi sebagai berikut:\n",
    "- Hasil analisa terhadap data, penanganan yang dilakukan serta justifikasi teknik-teknik yang dipilih\n",
    "- Perubahan yang dilakukan pada jawaban poin 1 - 5 jika ada\n",
    "- Desain eksperimen\n",
    "- Hasil eksperimen\n",
    "- Analisis dan kesimpulan\n",
    "- Pembagian Tugas / Kerja antar anggota kelompok\n",
    "\n",
    "Deadline pengumpulan:\n",
    "- Deliverables poin 1 dikumpulkan sebelum <b>pukul 11.00 WIB</b>, Senin 4 April 2022\n",
    "- Deliverables poin 2 dikumpulkan sebelum <b>pukul 23.59 WIB</b>, Senin 4 April 2022\n",
    "- Deliverables poin 3 dikumpulkan sebelum <b>pukul 23.59 WIB</b>, Senin 4 April 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
